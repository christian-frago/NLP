{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import classy_classification\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_text(all_text, filename):\n",
    "    #Open and Read all Training Data\n",
    "\n",
    "    with open ('trainset/Datatype/ArtisticProcess.txt', \"r\") as f:\n",
    "        ArtisticProcess = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/BestSellingBooks.txt', \"r\") as f:\n",
    "        BestSellingBooks = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/BlueprintsToolkit.txt', \"r\") as f:\n",
    "        BlueprintsToolkit = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/BusinessModels.txt', \"r\") as f:\n",
    "        BusinessModels = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/DiscoveryInvention.txt', \"r\") as f:\n",
    "        DiscoveryInvention = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/ExpertAdviceRecommentations.txt', \"r\") as f:\n",
    "        ExpertAdviceRecommentations = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/GovernanceStructure.txt', \"r\") as f:\n",
    "        GovernanceStructure = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/HealthRegimes.txt', \"r\") as f:\n",
    "        HealthRegimes = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/IndustryStandards.txt', \"r\") as f:\n",
    "        IndustryStandards = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/Lifestyle.txt', \"r\") as f:\n",
    "        Lifestyle = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/OperationsManual.txt', \"r\") as f:\n",
    "        OperationsManual = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/PhilosopyValues.txt', \"r\") as f:\n",
    "        PhilosopyValues = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/ProductionMethod.txt', \"r\") as f:\n",
    "        ProductionMethod = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/Regulations.txt', \"r\") as f:\n",
    "        Regulations = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/SolutionsPlaybooks.txt', \"r\") as f:\n",
    "        SolutionsPlaybooks = f.read().splitlines()\n",
    "\n",
    "    with open ('trainset/Datatype/Strategies.txt', \"r\") as f:\n",
    "        Strategies = f.read().splitlines()\n",
    "\n",
    "        \n",
    "    print(\"Data Type Classification Training data read!\")\n",
    "    \n",
    "    data = {}\n",
    "    data[\"Artistic Process\"] = ArtisticProcess\n",
    "    data[\"Best Selling Books\"] = BestSellingBooks\n",
    "    data[\"Blueprints Toolkit\"] = BlueprintsToolkit\n",
    "    data[\"Business Models\"] = BusinessModels\n",
    "    data[\"Discovery Invention\"] = DiscoveryInvention\n",
    "    data[\"Expert Advice Recommentations\"] = ExpertAdviceRecommentations\n",
    "    data[\"Governance Structure\"] = GovernanceStructure\n",
    "    data[\"Health Regimes\"] = HealthRegimes\n",
    "    data[\"Industry Standards\"] = IndustryStandards\n",
    "    data[\"Lifestyle\"] = Lifestyle\n",
    "    data[\"Operations Manual\"] = OperationsManual\n",
    "    data[\"Philosopy Values\"] = PhilosopyValues\n",
    "    data[\"Production Method\"] = ProductionMethod\n",
    "    data[\"Regulations\"] = Regulations\n",
    "    data[\"Solutions Playbooks\"] = SolutionsPlaybooks\n",
    "    data[\"Strategies\"] = Strategies\n",
    "\n",
    "    print(\"Applying Text Data Type Classification Model to :\", filename)\n",
    "    \n",
    "    #Apply Classy Model\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(\n",
    "        \"text_categorizer\", \n",
    "        config={\n",
    "            \"data\": data, \n",
    "            \"model\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"device\": \"cpu\"\n",
    "        }\n",
    "    ) \n",
    "    \n",
    "    print(\"Applying sentencizer!\")\n",
    "    sentence_model = spacy.blank(\"en\")\n",
    "    sentence_model.add_pipe(\"sentencizer\")\n",
    "    \n",
    "    print(\"Segmentation begin:\")\n",
    "    segment_size = 100000  # Define the desired segment size\n",
    "\n",
    "    num_segments = len(all_text) // segment_size + 1  # Calculate the number of segments\n",
    "\n",
    "    final_data = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = (i + 1) * segment_size\n",
    "        segment = all_text[start_idx:end_idx]\n",
    "\n",
    "        sentences = sentence_model(segment)\n",
    "\n",
    "        for sentence in sentences.sents:\n",
    "            doc = nlp(sentence.text)\n",
    "            final_data.append({\"sentence\": doc.text, \"cats\": doc._.cats})\n",
    "\n",
    "    # Extract the \"sentence\" and \"cats\" data from the final_data list, and trim spaces\n",
    "    sentences = [item[\"sentence\"].strip() for item in final_data]\n",
    "    categories_data = [item[\"cats\"] for item in final_data]\n",
    "\n",
    "    # Create the DataFrame with the \"Sentence\" column\n",
    "    df_cat = pd.DataFrame({\"Sentence\": sentences})\n",
    "\n",
    "    print(\"Composing the dataframe!\")\n",
    "          \n",
    "    # Add the category columns to the DataFrame\n",
    "    category_columns = [\n",
    "        \"Artistic Process\",\n",
    "        \"Best Selling Books\",\n",
    "        \"Blueprints Toolkit\",\n",
    "        \"Business Models\",\n",
    "        \"Discovery Invention\",\n",
    "        \"Expert Advice Recommentations\",\n",
    "        \"Governance Structure\",\n",
    "        \"Health Regimes\",\n",
    "        \"Industry Standards\",\n",
    "        \"Lifestyle\",\n",
    "        \"Operations Manual\",\n",
    "        \"Philosopy Values\",\n",
    "        \"Production Method\",\n",
    "        \"Regulations\",\n",
    "        \"Solutions Playbooks\",\n",
    "        \"Strategies\"\n",
    "    ]\n",
    "\n",
    "    for category in category_columns:\n",
    "        df_cat[category] = [data[category] for data in categories_data]\n",
    "\n",
    "    # Convert the probability values to percentages and round to two decimal places\n",
    "    df_cat[category_columns] = (df_cat[category_columns] * 100).round(6)\n",
    "\n",
    "    # Add a new column with the category_column name that has the highest value\n",
    "    df_cat['Category_Tag'] = df_cat[category_columns].idxmax(axis=1)\n",
    "\n",
    "    # Add a new column for the Filename\n",
    "    df_cat['Filename'] = filename\n",
    "\n",
    "    # Generate the CSV file path\n",
    "    csv_filename = os.path.join(\"dataset\", \"DataTypeClassification.csv\")\n",
    "\n",
    "    # Create the 'dataset' directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "\n",
    "    print(\"Creating / Updating Data Type CSV file\")\n",
    "          \n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(csv_filename):\n",
    "        # Load the existing CSV file into a DataFrame\n",
    "        df_existing = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Append the new data to the existing DataFrame\n",
    "        df_combined = pd.concat([df_existing, df_cat], ignore_index=True)\n",
    "    else:\n",
    "        # If the CSV file doesn't exist, just save the new DataFrame directly\n",
    "        df_combined = df_cat\n",
    "\n",
    "    # Save the combined DataFrame to the CSV file\n",
    "    df_combined.to_csv(csv_filename, index=False)\n",
    "    print(\"Data Type Classification completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python extract_data_type.py <all_text> <filename>\")\n",
    "    else:\n",
    "        all_text = sys.argv[1]\n",
    "        filename = sys.argv[2]\n",
    "        process_text(all_text, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
